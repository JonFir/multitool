use anyhow::{Context, Result};
use clap::Subcommand;
use llm_lib::{CompletionOptions, LlmClient, LlmConfig, Message};
use tracing::{info, instrument};

#[derive(Subcommand)]
pub enum LlmCommands {
    /// –û—Ç–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –∫ LLM
    Ask {
        /// –í–æ–ø—Ä–æ—Å –∏–ª–∏ –∑–∞–ø—Ä–æ—Å
        prompt: String,

        /// –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: anthropic/claude-3.5-sonnet)
        #[arg(short, long)]
        model: Option<String>,

        /// Temperature (0.0 - 2.0)
        #[arg(short, long)]
        temperature: Option<f32>,

        /// –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        #[arg(long)]
        max_tokens: Option<u32>,
    },

    /// –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–ª–∞–Ω —Ä–∞–±–æ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞—á –∏–∑ —Ç—Ä–µ–∫–µ—Ä–∞
    PlanDay {
        /// –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        #[arg(short, long)]
        model: Option<String>,
    },

    /// –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç —Å LLM
    Chat {
        /// –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        #[arg(short, long)]
        system: Option<String>,

        /// –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        #[arg(short, long)]
        model: Option<String>,
    },
}

impl LlmCommands {
    #[instrument(skip(self))]
    pub async fn execute(self) -> Result<()> {
        match self {
            LlmCommands::Ask {
                prompt,
                model,
                temperature,
                max_tokens,
            } => {
                let client = create_client(model)?;

                info!("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM");

                let response = if let (Some(temp), Some(tokens)) = (temperature, max_tokens) {
                    let options = CompletionOptions::new()
                        .temperature(temp)
                        .max_tokens(tokens);
                    let messages = vec![Message::user(&prompt)];
                    let completion = client.chat_completion(messages, Some(options)).await?;
                    completion
                        .content()
                        .context("No content in response")?
                        .to_string()
                } else {
                    client.complete(&prompt).await?
                };

                println!("\n{}\n", response);
                Ok(())
            }

            LlmCommands::PlanDay { model } => {
                plan_day(model).await?;
                Ok(())
            }

            LlmCommands::Chat { system, model } => {
                println!("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç (–ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω)");
                println!("System prompt: {:?}", system);
                println!("Model: {:?}", model.unwrap_or_default());
                Ok(())
            }
        }
    }
}

fn create_client(model: Option<String>) -> Result<LlmClient> {
    let api_key = std::env::var("OPEN_ROUTER_TOKEN")
        .context("OPEN_ROUTER_TOKEN environment variable not set")?;

    let model = model.unwrap_or_else(|| "anthropic/claude-3.5-sonnet".to_string());

    let config = LlmConfig {
        api_key,
        model,
        base_url: "https://openrouter.ai/api/v1".to_string(),
        timeout_secs: 120,
        site_url: Some("https://github.com/yourusername/you".to_string()),
        app_name: Some("you-cli".to_string()),
    };

    Ok(LlmClient::new(config)?)
}

#[instrument]
async fn plan_day(model: Option<String>) -> Result<()> {
    use tracker_lib::search::SearchRequest;
    use tracker_lib::TrackerClient;

    info!("–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–¥–∞—á –∏–∑ —Ç—Ä–µ–∫–µ—Ä–∞");

    // –ü–æ–ª—É—á–∏—Ç—å –∑–∞–¥–∞—á–∏ –∏–∑ —Ç—Ä–µ–∫–µ—Ä–∞
    let tracker = TrackerClient::from_env()
        .context("Failed to create tracker client. Make sure TRACKER_TOKEN is set")?;

    let request = SearchRequest::default();
    let issues = tracker
        .search_issues(&request, None)
        .await
        .context("Failed to fetch issues from tracker")?;

    if issues.is_empty() {
        println!("–ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è");
        return Ok(());
    }

    // –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á
    let task_list = issues
        .iter()
        .map(|issue| {
            let status_display = issue
                .status
                .as_ref()
                .and_then(|s| s.display.as_deref())
                .unwrap_or("Unknown");
            format!("- {} ({}): {}", issue.key, status_display, issue.summary)
        })
        .collect::<Vec<_>>()
        .join("\n");

    let prompt = format!(
        r#"–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–¥–∞—á, —Å–æ—Å—Ç–∞–≤—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω —Ä–∞–±–æ—Ç—ã –Ω–∞ –¥–µ–Ω—å:

{}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–ª–∞–Ω—É:
1. –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–π –∑–∞–¥–∞—á–∏ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
2. –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏
3. –£–∫–∞–∂–∏ –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –Ω–∞ –∫–∞–∂–¥—É—é –∑–∞–¥–∞—á—É
4. –î–æ–±–∞–≤—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–æ—Ä—è–¥–∫—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
5. –£—á—Ç–∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ –±–ª–æ–∫–µ—Ä—ã

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω –≤ —Ñ–æ—Ä–º–∞—Ç–µ markdown."#,
        task_list
    );

    // –ü–æ–ª—É—á–∏—Ç—å –ø–ª–∞–Ω –æ—Ç LLM
    info!("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ —Ä–∞–±–æ—Ç—ã");

    let client = create_client(model)?;
    let plan = client
        .complete_with_system(
            "–¢—ã - –æ–ø—ã—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–µ–∫—Ç–æ–≤, –ø–æ–º–æ–≥–∞—é—â–∏–π –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—á–∏–π –¥–µ–Ω—å",
            &prompt,
        )
        .await
        .context("Failed to generate plan")?;

    println!("\nüìã –ü–ª–∞–Ω —Ä–∞–±–æ—Ç—ã –Ω–∞ –¥–µ–Ω—å:\n");
    println!("{}\n", plan);

    Ok(())
}
